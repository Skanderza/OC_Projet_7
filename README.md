# Air Paradis - Analyse de Sentiments Twitter

Prototype IA pour anticiper le bad buzz sur les rÃ©seaux sociaux(Twitter)

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)]()

## Contexte du projet

### Le Client

**Air Paradis** une compagnie aÃ©rienne (fictive) qui fait face Ã  des dÃ©fis d'e-rÃ©putation sur les rÃ©seaux sociaux. Les bad buzz peuvent avoir un impact significatif sur l'image de marque et la confiance des clients.

### La Mission

Le cabinet **MIC (Marketing Intelligence Consulting)** a Ã©tÃ© missionnÃ© pour dÃ©velopper un prototype IA capable de :
- PrÃ©dire le sentiment(positif/nÃ©gatif) associÃ© Ã  un tweet
- Anticiper les bad buzz avant qu'ils ne deviennent viraux 
- Systeme d'alerte si 3  mauvaises predictions en moins de 5 minutes
- Fournir un outil accessible via une interface web simple

### Contraintes

- CoÃ»t de mise en production (solution Cloud gratuite Heroku)

## Objectifs

### 1. ModÃ©lisation
Comparer plusieurs approches de Machine Learning et Deep Learning :
- **ModÃ¨le simple** : Logistic Regression (TF-IDF/Word2vec/GloVe/USE)
- **ModÃ¨le Deep learning simple** : Couche embedding + (Word2vec/GloVe/USE)
- **ModÃ¨le Deep learning avancÃ©** : LSTM/LSTM bidirectionnel/Distilbert(Transfert learning)

### 2. MLOps
Mettre en Å“uvre une dÃ©marche MLOps complÃ¨te :
- **Tracking** des expÃ©rimentations avec MLflow
- **Pipeline CI/CD** avec GitHub Actions
- **Monitoring** en production avec Azure Application Insights
- **Alertes** automatiques en cas de mauvaises predictions

### 3. DÃ©ploiement
- **API REST** de prÃ©diction (FastAPI)
- **Interface utilisateur** (Streamlit)
- **Feedback loop** pour l'amÃ©lioration continue

---

## Architecture Globale

L'architecture suit un flux complet de bout en bout :

1. **Interface utilisateur** (Streamlit local) â†’ Saisie du tweet
2. **API REST** (FastAPI sur Heroku) â†’ Traitement et prÃ©diction
3. **ModÃ¨le ML** (TF-IDF + LogReg) â†’ Classification du sentiment
4. **Monitoring** (Azure Application Insights) â†’ Suivi des performances et alertes

---

## DonnÃ©es

| CaractÃ©ristique | DÃ©tail |
|-----------------|--------|
| **Source** | Sentiment140 Dataset |
| **Volume** | 1.6 million de tweets |
| **Volume traitÃ©** | 100k de tweets |
| **Format** | Label binaire (0: nÃ©gatif, 1: positif) |
| **TÃ©lÃ©chargement** | [Kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140) |

---
## Structure du Repository

```
OC_Projet_7/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # CI/CD GitHub Actions
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models_lr/          # ModÃ¨les Logistic Regression
â”‚   â”‚   â”œâ”€â”€ logreg_tfidf_133k.joblib
â”‚   â”‚   â””â”€â”€ tfidf_vectorizer_133k.joblib
â”‚   â”œâ”€â”€ .env.example        # Template variables d'environnement
â”‚   â”œâ”€â”€ Model.py            # Classe modÃ¨le de prÃ©diction
â”‚   â”œâ”€â”€ Model_LR.py         # ModÃ¨le Logistic Regression
â”‚   â”œâ”€â”€ Tweets.py           # SchÃ©mas Pydantic
â”‚   â”œâ”€â”€ app.py              # Application FastAPI
â”‚   â”œâ”€â”€ monitoring.py       # IntÃ©gration Azure Insights
â”‚   â””â”€â”€ streamlit_app.py    # Interface utilisateur
â”‚   â””â”€â”€ logo.webp    # Logo app
â”œâ”€â”€ models_final/           # ModÃ¨les finaux versionnÃ©s
â”‚   â”œâ”€â”€ logreg_tfidf_133k.joblib
â”‚   â””â”€â”€ tfidf_vectorizer_133k.joblib
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ p7_EDA.ipynb                # Analyse exploratoire
â”‚   â”œâ”€â”€ p7_modele_simple.ipynb      # Regression logistique
â”‚   â”œâ”€â”€ p7_DL_simple.ipynb          # Deep Learning simple
â”‚   â”œâ”€â”€ p7_DL_avance.ipynb          # Deep Learning avancÃ©
â”‚   â””â”€â”€ test_model_distilBert.ipynb # ExpÃ©rimentation BERT
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_app.py             # Tests API
â”‚   â”œâ”€â”€ test_model_lr.py        # Tests modÃ¨le LogReg
â”‚   â””â”€â”€ test_model_distilbert.py # Tests modÃ¨le BERT
â”œâ”€â”€ presentation/
â”‚   â””â”€â”€BLOG.md                 # Article blog MLOps
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ CHANGELOG.md            # Historique des versions
â”œâ”€â”€ Dockerfile              # Configuration Docker
â”œâ”€â”€ Procfile                # Configuration Heroku
â”œâ”€â”€ README.md               # Ce fichier
â”œâ”€â”€ model_tf_lite.py        # Script conversion TF Lite
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â””â”€â”€ runtime.txt             # Version Python Heroku
```
---

## Quick Start

### PrÃ©requis

- Python 3.11+
- Conda (recommandÃ©) ou pip
- Git

### Installation

```bash
# Cloner le repository
git clone https://github.com/Skanderza/OC_Projet_7.git
cd OC_Projet_7

# CrÃ©er l'environnement conda
conda create -n p7_sentiment python=3.11
conda activate p7_sentiment

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Lancement Local

```bash
# Lancer l'API
uvicorn app.app:app --reload --port 8000

# Dans un autre terminal, lancer Streamlit
streamlit run app/streamlit/app.py
```

### AccÃ¨s

- **Interface Streamlit** : http://localhost:8501
- **API locale** : http://localhost:8000/docs
- **MLflow UI** : http://localhost:5000
- **Heroku** : https://sentiment-twitter-p7-357ab866923c.herokuapp.com/docs (Dyno deconnectÃ©)

---

## RÃ©sultats ClÃ©s

| ModÃ¨le | Accuracy | F1-Score | Precision | DÃ©ployÃ© |
|--------|----------|----------|---------|:-------:|
| TF-IDF + Logistic Regression | 0.795 | 0.795 | 0.794 | âœ… |
| DistilBERT | 0.809 | 0.795| 0.854 | âŒ |


**Choix du modÃ¨le de production** : TF-IDF + Logistic Regression
- Performances solides
- Compatible avec les contraintes mÃ©moire Heroku (512 MB)
- Temps d'infÃ©rence rapide

â†’ DÃ©tails complets dans [notebook/README.md](notebook/README.md)

---

## Documentation

| Document | Description |
|----------|-------------|
| [notebook/README.md](notebook/README.md) | Approches de modÃ©lisation et rÃ©sultats dÃ©taillÃ©s |
| [app/README.md](app/README.md) | Documentation API et Interface utilisateur |
| [BLOG.md](BLOG.md) | Article blog sur la dÃ©marche MLOps |
| [CHANGELOG.md](CHANGELOG.md) | Historique des versions |

---

## ğŸ‘¤ Auteur

**Skander ZAHI**

Ce projet est rÃ©alisÃ© dans le cadre de la formation **OpenClassrooms - Parcours Data Scientist**.

Dataset disponible publiquement sur Kaggle.

---